{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Model Training Guide - Hub and Spoke Architecture\n",
        "\n",
        "This notebook demonstrates how to train the Hub and Spoke models for the Enterprise Fraud Detection System.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Training Data Preparation](#data-prep)\n",
        "2. [Hub Model Training](#hub-training)\n",
        "3. [Spoke Model Training](#spoke-training)\n",
        "4. [Model Evaluation](#evaluation)\n",
        "5. [Hyperparameter Optimization](#optimization)\n",
        "6. [Model Deployment](#deployment)\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "The training process involves:\n",
        "- **Hub Model**: Trains on profile, behavioral, and network features to provide unified customer risk scores\n",
        "- **Spoke Models**: Train on contextual features + hub scores to provide product-specific fraud detection\n",
        "\n",
        "**Training Flow:**\n",
        "```\n",
        "Raw Data ‚Üí Feature Engineering ‚Üí Hub Model Training ‚Üí Spoke Model Training ‚Üí Evaluation ‚Üí Deployment\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add the src directory to Python path\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Import our custom modules\n",
        "from utils.config_manager import ConfigManager\n",
        "from models.hub_model import HubModelManager, XGBoostHubModel, ModelTrainingConfig\n",
        "from models.spoke_models import SpokeModelManager, PIXSpokeModel, CreditCardSpokeModel\n",
        "from features.feature_store import FeatureStore\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üîß Training Environment Setup Complete\")\n",
        "print(f\"üìÖ Training Session: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Initialize configuration\n",
        "config = ConfigManager()\n",
        "print(f\"‚úÖ Configuration loaded for environment: {config.environment}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Training Data Preparation\n",
        "\n",
        "First, we need to prepare training datasets for both Hub and Spoke models. In a real scenario, this data would come from your data warehouse, but here we'll generate synthetic data that resembles production patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive training data for Hub model\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_hub_training_data(n_samples=10000):\n",
        "    \"\"\"Generate synthetic training data for Hub model\"\"\"\n",
        "    \n",
        "    print(f\"üèóÔ∏è Generating {n_samples:,} training samples for Hub model...\")\n",
        "    \n",
        "    # Generate customer IDs\n",
        "    customer_ids = [f'cust_{i:08d}' for i in range(n_samples)]\n",
        "    \n",
        "    # Pillar 1: Profile Features\n",
        "    profile_features = {\n",
        "        'customer_age': np.random.normal(40, 15, n_samples).clip(18, 80).astype(int),\n",
        "        'account_age_days': np.random.exponential(800, n_samples).clip(1, 3650).astype(int),\n",
        "        'total_products_count': np.random.poisson(2.8, n_samples).clip(1, 8),\n",
        "        'credit_score_internal': np.random.normal(650, 120, n_samples).clip(300, 850).astype(int),\n",
        "        'is_pep': np.random.choice([0, 1], n_samples, p=[0.98, 0.02]),\n",
        "        'kyc_completion_score': np.random.beta(4, 1.5, n_samples),  # Skewed towards completion\n",
        "    }\n",
        "    \n",
        "    # Pillar 2: Behavioral Features (various time windows)\n",
        "    behavioral_features = {}\n",
        "    time_windows = ['1h', '6h', '24h', '7d', '30d']\n",
        "    \n",
        "    for window in time_windows:\n",
        "        # Transaction counts and volumes\n",
        "        behavioral_features[f'transaction_count_{window}'] = np.random.poisson(\n",
        "            {'1h': 0.5, '6h': 2, '24h': 8, '7d': 45, '30d': 180}[window], n_samples\n",
        "        )\n",
        "        \n",
        "        behavioral_features[f'transaction_volume_{window}'] = np.random.lognormal(\n",
        "            {'1h': 4, '6h': 5.5, '24h': 7, '7d': 9, '30d': 11}[window], 1.2, n_samples\n",
        "        )\n",
        "        \n",
        "        behavioral_features[f'channels_used_{window}'] = np.random.poisson(\n",
        "            {'1h': 1, '6h': 1.2, '24h': 1.8, '7d': 2.5, '30d': 2.8}[window], n_samples\n",
        "        ).clip(0, 4)\n",
        "    \n",
        "    # Digital behavior\n",
        "    behavioral_features.update({\n",
        "        'login_count_7d': np.random.poisson(12, n_samples),\n",
        "        'avg_session_duration_30d': np.random.exponential(20, n_samples),  # minutes\n",
        "        'password_changes_90d': np.random.poisson(0.3, n_samples),\n",
        "    })\n",
        "    \n",
        "    # Pillar 3: Network Features\n",
        "    network_features = {\n",
        "        'customers_sharing_devices': np.random.poisson(0.8, n_samples).clip(0, 20),\n",
        "        'unique_devices_used': np.random.poisson(2.2, n_samples).clip(1, 10),\n",
        "        'unique_beneficiaries': np.random.poisson(8, n_samples),\n",
        "        'fraudulent_beneficiaries_count': np.random.poisson(0.1, n_samples),\n",
        "        'network_out_degree': np.random.poisson(5, n_samples),\n",
        "        'network_in_degree': np.random.poisson(3, n_samples),\n",
        "    }\n",
        "    \n",
        "    # Combine all features\n",
        "    all_features = {**profile_features, **behavioral_features, **network_features}\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(all_features, index=customer_ids)\n",
        "    \n",
        "    # Generate fraud labels based on realistic patterns\n",
        "    fraud_prob = (\n",
        "        0.01 +  # Base fraud rate\n",
        "        0.02 * (df['credit_score_internal'] < 500) +  # Low credit score\n",
        "        0.03 * (df['is_pep'] == 1) +  # PEP customers\n",
        "        0.01 * (df['customers_sharing_devices'] > 5) +  # Device sharing\n",
        "        0.02 * (df['fraudulent_beneficiaries_count'] > 0) +  # Risky network\n",
        "        0.01 * (df['kyc_completion_score'] < 0.5) +  # Incomplete KYC\n",
        "        0.02 * (df['transaction_count_24h'] > df['transaction_count_24h'].quantile(0.95))  # High activity\n",
        "    )\n",
        "    \n",
        "    df['is_fraud'] = np.random.binomial(1, fraud_prob.clip(0, 0.8))\n",
        "    \n",
        "    # Add timestamp for time series split\n",
        "    df['label_timestamp'] = pd.date_range(\n",
        "        start='2022-01-01', \n",
        "        end='2024-01-01', \n",
        "        periods=n_samples\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Generated Hub training data:\")\n",
        "    print(f\"   üìä Shape: {df.shape}\")\n",
        "    print(f\"   üö® Fraud rate: {df['is_fraud'].mean():.2%}\")\n",
        "    print(f\"   üìÖ Date range: {df['label_timestamp'].min()} to {df['label_timestamp'].max()}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Generate training data\n",
        "hub_training_data = generate_hub_training_data(10000)\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nüìã Sample Hub Training Data:\")\n",
        "print(hub_training_data.head(10).round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hub Model Training\n",
        "\n",
        "The Hub model is trained on profile, behavioral, and network features to provide a unified customer risk assessment. This model answers the question: \"What is the overall fraud risk of this customer right now?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Hub Model\n",
        "print(\"üéØ Starting Hub Model Training\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Prepare features and target\n",
        "feature_cols = [col for col in hub_training_data.columns \n",
        "                if col not in ['is_fraud', 'label_timestamp']]\n",
        "\n",
        "X = hub_training_data[feature_cols].copy()\n",
        "y = hub_training_data['is_fraud'].copy()\n",
        "\n",
        "print(f\"üìä Training set shape: {X.shape}\")\n",
        "print(f\"üéØ Target distribution: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# Time series split to respect temporal order\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "train_idx, val_idx = list(tscv.split(X))[-1]  # Use last split\n",
        "\n",
        "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "print(f\"üìà Training set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"üìä Validation set: {X_val.shape[0]:,} samples\")\n",
        "\n",
        "# Create and train Hub model\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "# For demonstration, we'll use scikit-learn instead of the complex XGBoost setup\n",
        "hub_model = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"\\nüîÑ Training Hub Model...\")\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Train the model\n",
        "hub_model.fit(X_train, y_train)\n",
        "\n",
        "training_time = (datetime.now() - start_time).total_seconds()\n",
        "print(f\"‚úÖ Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "# Evaluate the model\n",
        "y_train_pred = hub_model.predict_proba(X_train)[:, 1]\n",
        "y_val_pred = hub_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "train_auc = roc_auc_score(y_train, y_train_pred)\n",
        "val_auc = roc_auc_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"\\nüìä Hub Model Performance:\")\n",
        "print(f\"   Training AUC: {train_auc:.4f}\")\n",
        "print(f\"   Validation AUC: {val_auc:.4f}\")\n",
        "print(f\"   Overfitting: {train_auc - val_auc:.4f}\")\n",
        "\n",
        "# Feature importance analysis\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': hub_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nüîç Top 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Hub Model Training Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Feature Importance\n",
        "top_features = feature_importance.head(15)\n",
        "axes[0, 0].barh(range(len(top_features)), top_features['importance'])\n",
        "axes[0, 0].set_yticks(range(len(top_features)))\n",
        "axes[0, 0].set_yticklabels(top_features['feature'])\n",
        "axes[0, 0].set_title('Top 15 Feature Importances')\n",
        "axes[0, 0].set_xlabel('Importance')\n",
        "\n",
        "# 2. Score Distribution by Class\n",
        "axes[0, 1].hist(y_train_pred[y_train == 0], bins=50, alpha=0.7, label='Legitimate', density=True)\n",
        "axes[0, 1].hist(y_train_pred[y_train == 1], bins=50, alpha=0.7, label='Fraud', density=True)\n",
        "axes[0, 1].set_title('Hub Model Score Distribution')\n",
        "axes[0, 1].set_xlabel('Fraud Score')\n",
        "axes[0, 1].set_ylabel('Density')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred)\n",
        "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_pred)\n",
        "\n",
        "axes[1, 0].plot(fpr_train, tpr_train, label=f'Training (AUC = {train_auc:.3f})', linewidth=2)\n",
        "axes[1, 0].plot(fpr_val, tpr_val, label=f'Validation (AUC = {val_auc:.3f})', linewidth=2)\n",
        "axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "axes[1, 0].set_title('ROC Curve')\n",
        "axes[1, 0].set_xlabel('False Positive Rate')\n",
        "axes[1, 0].set_ylabel('True Positive Rate')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Precision-Recall Curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision_train, recall_train, _ = precision_recall_curve(y_train, y_train_pred)\n",
        "precision_val, recall_val, _ = precision_recall_curve(y_val, y_val_pred)\n",
        "\n",
        "axes[1, 1].plot(recall_train, precision_train, label='Training', linewidth=2)\n",
        "axes[1, 1].plot(recall_val, precision_val, label='Validation', linewidth=2)\n",
        "axes[1, 1].set_title('Precision-Recall Curve')\n",
        "axes[1, 1].set_xlabel('Recall')\n",
        "axes[1, 1].set_ylabel('Precision')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Model performance at different thresholds\n",
        "thresholds = [0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9]\n",
        "performance_metrics = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_binary = (y_val_pred >= threshold).astype(int)\n",
        "    \n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "    \n",
        "    precision = precision_score(y_val, y_pred_binary, zero_division=0)\n",
        "    recall = recall_score(y_val, y_pred_binary, zero_division=0)\n",
        "    f1 = f1_score(y_val, y_pred_binary, zero_division=0)\n",
        "    \n",
        "    performance_metrics.append({\n",
        "        'threshold': threshold,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(performance_metrics)\n",
        "print(f\"\\nüéØ Performance at Different Thresholds:\")\n",
        "print(metrics_df.round(3))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
