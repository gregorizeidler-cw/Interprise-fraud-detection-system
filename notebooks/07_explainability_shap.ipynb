{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸ” Advanced Explainability with SHAP for Fraud Detection\n",
        "\n",
        "## ğŸ¯ Overview\n",
        "\n",
        "This notebook provides a comprehensive guide to **SHAP (SHapley Additive exPlanations)** for fraud detection model interpretability. SHAP is the gold standard for explaining machine learning predictions, especially important for regulatory compliance and business transparency.\n",
        "\n",
        "### ğŸ” What You'll Learn\n",
        "\n",
        "1. **ğŸ§  SHAP Fundamentals**: Understanding Shapley values and their importance\n",
        "2. **âš¡ SHAP Explainers**: TreeExplainer, KernelExplainer, and DeepExplainer\n",
        "3. **ğŸ“Š Global Explanations**: Understanding model behavior across all predictions\n",
        "4. **ğŸ¯ Local Explanations**: Explaining individual fraud predictions\n",
        "5. **ğŸ“ˆ Visualization**: Waterfall plots, summary plots, and force plots\n",
        "6. **ğŸ¦ Business Applications**: Regulatory compliance and decision support\n",
        "\n",
        "### ğŸŒŸ Key Benefits\n",
        "\n",
        "- **âš–ï¸ Regulatory Compliance**: Meet explainability requirements (GDPR, Basel III)\n",
        "- **ğŸ¯ Model Debugging**: Identify biases and unexpected patterns\n",
        "- **ğŸ“Š Feature Insights**: Understand which features drive fraud predictions\n",
        "- **ğŸ¤ Stakeholder Communication**: Explain decisions to business users\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ—ï¸ SHAP Architecture\n",
        "\n",
        "```\n",
        "ğŸ¤– Trained Fraud Model\n",
        "        â†“\n",
        "ğŸ” SHAP Explainer (Tree/Kernel/Deep)\n",
        "        â†“\n",
        "ğŸ“Š Shapley Values (Feature Contributions)\n",
        "        â†“\n",
        "ğŸ“ˆ Visualizations + ğŸ“ Natural Language Explanations\n",
        "        â†“\n",
        "âš–ï¸ Compliance Reports + ğŸ¯ Business Actions\n",
        "```\n",
        "\n",
        "### ğŸ“š SHAP Theory Overview\n",
        "\n",
        "**Shapley Values** come from cooperative game theory and provide the **only** explanation method that satisfies these important properties:\n",
        "\n",
        "- **ğŸ¯ Efficiency**: All feature contributions sum to the prediction difference\n",
        "- **âš–ï¸ Symmetry**: Features with identical impact get identical contributions\n",
        "- **ğŸ”„ Dummy**: Features that don't affect the model get zero contribution\n",
        "- **ğŸ“Š Additivity**: Consistent across different models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# SHAP for explainability\n",
        "try:\n",
        "    import shap\n",
        "    print(\"âœ… SHAP imported successfully!\")\n",
        "    shap.initjs()  # Initialize JavaScript for SHAP visualizations\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ SHAP not found. Install with: pip install shap\")\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ğŸ” Advanced SHAP Explainability for Fraud Detection\")\n",
        "print(\"=\" * 55)\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"ğŸ“… Notebook initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸ” Advanced Explainability with SHAP for Fraud Detection\n",
        "\n",
        "## ğŸ¯ Overview\n",
        "\n",
        "This notebook provides a comprehensive guide to **SHAP (SHapley Additive exPlanations)** for fraud detection model interpretability. SHAP is the gold standard for explaining machine learning predictions, especially important for regulatory compliance and business transparency.\n",
        "\n",
        "### ğŸ” What You'll Learn\n",
        "\n",
        "1. **ğŸ§  SHAP Fundamentals**: Understanding Shapley values and their importance\n",
        "2. **âš¡ SHAP Explainers**: TreeExplainer, KernelExplainer, and DeepExplainer\n",
        "3. **ğŸ“Š Global Explanations**: Understanding model behavior across all predictions\n",
        "4. **ğŸ¯ Local Explanations**: Explaining individual fraud predictions\n",
        "5. **ğŸ“ˆ Visualization**: Waterfall plots, summary plots, and force plots\n",
        "6. **ğŸ¦ Business Applications**: Regulatory compliance and decision support\n",
        "\n",
        "### ğŸŒŸ Key Benefits\n",
        "\n",
        "- **âš–ï¸ Regulatory Compliance**: Meet explainability requirements (GDPR, Basel III)\n",
        "- **ğŸ¯ Model Debugging**: Identify biases and unexpected patterns\n",
        "- **ğŸ“Š Feature Insights**: Understand which features drive fraud predictions\n",
        "- **ğŸ¤ Stakeholder Communication**: Explain decisions to business users\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ—ï¸ SHAP Architecture\n",
        "\n",
        "```\n",
        "ğŸ¤– Trained Fraud Model\n",
        "        â†“\n",
        "ğŸ” SHAP Explainer (Tree/Kernel/Deep)\n",
        "        â†“\n",
        "ğŸ“Š Shapley Values (Feature Contributions)\n",
        "        â†“\n",
        "ğŸ“ˆ Visualizations + ğŸ“ Natural Language Explanations\n",
        "        â†“\n",
        "âš–ï¸ Compliance Reports + ğŸ¯ Business Actions\n",
        "```\n",
        "\n",
        "### ğŸ“š SHAP Theory Overview\n",
        "\n",
        "**Shapley Values** come from cooperative game theory and provide the **only** explanation method that satisfies these important properties:\n",
        "\n",
        "- **ğŸ¯ Efficiency**: All feature contributions sum to the prediction difference\n",
        "- **âš–ï¸ Symmetry**: Features with identical impact get identical contributions\n",
        "- **ğŸ”„ Dummy**: Features that don't affect the model get zero contribution\n",
        "- **ğŸ“Š Additivity**: Consistent across different models\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
